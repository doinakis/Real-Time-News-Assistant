\section{Μοντέλο BERT}
Το μοντέλο Αμφίδρομης Αναπαράστασης Κωδικοποιητών από \emph{Transformers} \linebreak(\emph{Bidirectional Encoder Representation from Transformers - BERT}) \cite{bert} αποτελεί ένα προ-εκπαιδευμένο μοντέλο που εστιάζει στην επίλυση προβλημάτων \emph{NLP} και \emph{NLU}. Η αρχιτεκτονική του είναι βασισμένη στα \emph{transformers} και πιο συγκεκριμένα στους \emph{encoders} τους. Στο άρθρο τους \cite{bert} οι \emph{Devlin et al.} παρουσιάζουν δύο παραλλαγές του μοντέλου το $BERT_{BASE}$ και το $BERT_{LARGE}$, με το πρώτο να αποτελείται από 12 \emph{encoders}, 12 \emph{attention heads} και 768 \emph{hidden layers} ενώ το δεύτερο από 24, 16 και 1024 αντίστοιχα.

Για να μπορέσει το \emph{BERT} να χρησιμοποιηθεί σε πληθώρα γλωσσικών προβλημάτων η είσοδος του μπορεί να είναι είτε μία πρόταση είτε ζεύγος προτάσεων, οι οποίες συνδέονται νοηματικά. Η προ-εκπαίδευση του χωρίστηκε σε δύο κατηγορίες προβλημάτων, την συμπλήρωση κενής λέξης (\emph{Masked Language Modeling - MLM}) και την πρόβλεψη επόμενης πρότασης (\emph{Next Sequence Prediction - NSP}). Πιο συγκεκριμένα, τα προβλήματα τύπου \emph{MLM} εστιάζουν στην τυχαία απόκρυψη κάποιον λέξεων της πρότασης εισόδου. Ως πρόβλημα προς επίλυση το μοντέλο πρέπει να προβλέψει τις αντίστοιχες κρυφές λέξεις. Από την άλλη μεριά τα προβλήματα \emph{NSP} εστιάζουν στην πρόβλεψη της επόμενης πρότασης, δεδομένου μιας πρότασης εισόδου. Αυτού του είδους η προ-εκπαίδευση είναι βοηθητική κυρίως σε προβλήματα τα οποία προσπαθούν να προσδιορίσουν την νοηματική σχέση μεταξύ δύο προτάσεων. Παράδειγμα τέτοιων προβλημάτων αποτελούν τα συστήματα \emph{QA}.

Ο τρόπος με τον οποίο το \emph{BERT} μπορεί να χρησιμοποιηθεί σε πληθώρα προβλημάτων είναι η επανεκπαίδευση (\emph{fine-tuning}) του σε κάποιο συγκεκριμένο πρόβλημα. Το \emph{fine-tuning} του μπορεί να επιτευχθεί με την παροχή των κατάλληλων εισόδων και εξόδων στη διαδικασία της επανεκπαίδευσης. Με τον τρόπο αυτό το μοντέλο έχει τη δυνατότητα να επιλύσει προβλήματα QA, κατηγοριοποίησης κειμένου, μετάφρασης και άλλων. Επιπλέον, ακριβώς επειδή έχει αρχικά εκπαιδευτεί σε πιο γενικά αλλά παρόμοια προβλήματα η διαδικασία του \emph{fine-tuning} είναι σχετικά γρήγορη, σε σχέση με το αν γινόταν εκπαίδευση του μοντέλου από το μηδέν.

\section{Μοντέλο Greek-Bert}
Μετά την αρκετά μεγάλη επιρροή του \emph{BERT} στο χώρο της επεξεργασίας φυσικής γλώσσας έγιναν μελέτες του μοντέλου και σε γλώσσες διαφορετικές από τα αγγλικά. Το $2020$, παρουσιάστηκε η αντίστοιχη προσπάθεια για την ελληνική γλώσσα στο άρθρο \emph{GREEK-BERT: The Greeks visiting Sesame Street} \cite{greek-bert} από τους \emph{Koutsikakis et al.} όπου μελετήθηκαν οι αντίστοιχες αρχιτεκτονικές $BERT_{BASED}$ και $BERT_{LARGE}$, με την εκπαίδευση των μοντέλων σε μεγάλο σύνολο δεδομένων της ελληνικής γλώσσας. Η αξιολόγηση των δύο μοντέλων έγινε στα παρακάτω προβλήματα:
\begin{itemize}
    \item Συντακτική Ανάλυση Προτάσεων (\emph{Part of Speech tagging - PoS tagging})
    \item Εξαγωγή οντοτήτων (\emph{Named Entity Recognition - NER})
    \item Εξαγωγή συμπερασμάτων από φυσική γλώσσα (\emph{Natural Language Inference - NLI})
\end{itemize}

Η σύγκριση του έγινε με άλλα μοντέλα τελευταίας τεχνολογίας και η απόδοση του αποδείχθηκε καλύτερη στα \emph{NER} και \emph{NLI}.